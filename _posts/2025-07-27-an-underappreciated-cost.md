---
title:  An Underappreciated Cost of AI
date:   2025-07-27 20:37:35 -0500
categories: ai
---


# {{ page.title }}


## Three Concerns 


1. The first concern is a broad fear that our obsession with optimizing for measurable goals is a vulnerability which makes us more easily influenced and manipulated.  An AI system that interacts with a wide swath of humanity (as ChatGPT and its ilk do) can have much more influence if we largely want the same things, especially when those things are easy to measure. All that lies between any agent (artificial or otherwise) and absolute power are the roles, resources, and minds of ordinary people---and the easier people are to manipulate (and in similar ways), the more likely it is that we accidentally cede control to an AI system.

2. The second concern is less dramatic, but, to my mind, more likely.  Beyond the obvious levers of power, I'm concerned that our (optimization-based) economic system could become entangled with an AI's assessment of value, leading to a situation in which people are trapped working against their own values. Of course, this is already a problem (e.g., tragedies of the commons; forced labor in scam call centers in Cambodia), and one brought about by misalignment between the incentives of agents at different scales.  I am concerned that even a narrowly capable AI optimizer could provide enough benefit to key actors to leave us permanently in a more "thermodynamically stable" but dystopian situation, entrenching itself in our social and economic fabric by "spending down" slack in our culture. 

3. My final concern is that, in outsourcing thinking to an AI system (even an optimal one), we risk losing the resilience that comes from diversity of thought, leading to the same brittleness as of a population lacking biodiversity. In particular, the effect could well feed into and exacerbate concerns 1 and 2.
Despite all this, I feel a certain degree of optimism is also necessary to navigate us safely to a future with powerful AI systems. Fear can be blinding and paralyzing.  I worry it will be hard to get things right if we spend all our time imagining only what could go wrong.


**Truth-based AI.**
I agree with your position that a powerful (truth-based) oracle is not, in general, meaningfully safer than an arbitrary agent with its own values and agenda.  However, part of this is because an oracle is not purely about truth, but also entails a commitment to provide answers (thereby potentially serving the desires of the questioner).  Think of the Buddhist position: that desire is the root of suffering.  We do not view monks as threatening (even if we believe them to be far more intelligent than we are) because we trust them not only to keep the knowledge, but also with the wisdom to know what is worth saying.  We do not expect them, for instance, to tell a distraught person how to make a bomb if they ask.  And part of the reason we trust monks, as well as scientists for that matter, is their commitment to truth and neutrality.  For these reasons, I still think "truth-based" approaches to AI are not necessarily equivalent to arbitrary loss-minimizing approaches. After all, centuries of philosophy have documented fundamental differences between theoretical and practical reasoning.

If everyone uses a calculator, we all get the same result, which is a good thing. 